
@misc{chen_unleashing_2023,
	title = {Unleashing the potential of prompt engineering in {Large} {Language} {Models}: a comprehensive review},
	shorttitle = {Unleashing the potential of prompt engineering in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.14735},
	doi = {10.48550/arXiv.2310.14735},
	abstract = {This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we gather information about the application of prompt engineering in such fields as education and programming, showing its transformative potential. This comprehensive survey aims to serve as a friendly guide for anyone venturing through the big world of LLMs and prompt engineering.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Chen, Banghao and Zhang, Zhaofeng and Langren√©, Nicolas and Zhu, Shengxin},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14735 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7},
}

@misc{lee_cxr-llava_2023,
	title = {{CXR}-{LLaVA}: {Multimodal} {Large} {Language} {Model} for {Interpreting} {Chest} {X}-ray {Images}},
	shorttitle = {{CXR}-{LLaVA}},
	url = {http://arxiv.org/abs/2310.18341},
	doi = {10.48550/arXiv.2310.18341},
	abstract = {Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling. Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at https://github.com/ECOFRI/CXR\_LLaVA. Results: In the test set, we observed that the model's performance fluctuated based on its parameters. On average, it achieved F1 score of 0.34 for five pathologic findings (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion), which was improved to 0.46 through prompt engineering. In the independent set, the model achieved an average F1 score of 0.30 for the same pathologic findings. Notably, for the pediatric chest radiograph dataset, which was unseen during training, the model differentiated abnormal radiographs with an F1 score ranging from 0.84 to 0.85. Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation. Both prompt engineering and model parameter adjustments can play pivotal roles in interpreting CXRs.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Lee, Seowoo and Youn, Jiwon and Kim, Mansu and Yoon, Soon Ho},
	month = oct,
	year = {2023},
	note = {arXiv:2310.18341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wan_exploring_2023,
	title = {Exploring {Generative} {AI} assisted feedback writing for students' written responses to a physics conceptual question with prompt engineering and few-shot learning},
	url = {http://arxiv.org/abs/2311.06180},
	doi = {10.48550/arXiv.2311.06180},
	abstract = {Instructor's feedback plays a critical role in students' development of conceptual understanding and reasoning skills. However, grading student written responses and providing personalized feedback can take a substantial amount of time. In this study, we explore using GPT-3.5 to write feedback to student written responses to conceptual questions with prompt engineering and few-shot learning techniques. In stage one, we used a small portion (n=20) of the student responses on one conceptual question to iteratively train GPT. Four of the responses paired with human-written feedback were included in the prompt as examples for GPT. We tasked GPT to generate feedback to the other 16 responses, and we refined the prompt after several iterations. In stage two, we gave four student researchers the 16 responses as well as two versions of feedback, one written by the authors and the other by GPT. Students were asked to rate the correctness and usefulness of each feedback, and to indicate which one was generated by GPT. The results showed that students tended to rate the feedback by human and GPT equally on correctness, but they all rated the feedback by GPT as more useful. Additionally, the successful rates of identifying GPT's feedback were low, ranging from 0.1 to 0.6. In stage three, we tasked GPT to generate feedback to the rest of the student responses (n=65). The feedback was rated by four instructors based on the extent of modification needed if they were to give the feedback to students. All the instructors rated approximately 70\% of the feedback statements needing only minor or no modification. This study demonstrated the feasibility of using Generative AI as an assistant to generating feedback for student written responses with only a relatively small number of examples. An AI assistance can be one of the solutions to substantially reduce time spent on grading student written responses.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Wan, Tong and Chen, Zhongzhou},
	month = nov,
	year = {2023},
	note = {arXiv:2311.06180 [physics]},
	keywords = {Physics - Physics Education},
}

@misc{van_zandvoort_enhancing_2023,
	title = {Enhancing {Summarization} {Performance} through {Transformer}-{Based} {Prompt} {Engineering} in {Automated} {Medical} {Reporting}},
	url = {http://arxiv.org/abs/2311.13274},
	doi = {10.48550/arXiv.2311.13274},
	abstract = {Customized medical prompts enable Large Language Models (LLM) to effectively address medical dialogue summarization. The process of medical reporting is often time-consuming for healthcare professionals. Implementing medical dialogue summarization techniques presents a viable solution to alleviate this time constraint by generating automated medical reports. The effectiveness of LLMs in this process is significantly influenced by the formulation of the prompt, which plays a crucial role in determining the quality and relevance of the generated reports. In this research, we used a combination of two distinct prompting strategies, known as shot prompting and pattern prompting to enhance the performance of automated medical reporting. The evaluation of the automated medical reports is carried out using the ROUGE score and a human evaluation with the help of an expert panel. The two-shot prompting approach in combination with scope and domain context outperforms other methods and achieves the highest score when compared to the human reference set by a general practitioner. However, the automated reports are approximately twice as long as the human references, due to the addition of both redundant and relevant statements that are added to the report.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {van Zandvoort, Daphne and Wiersema, Laura and Huibers, Tom and van Dulmen, Sandra and Brinkkemper, Sjaak},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13274 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{parnami_learning_2022,
	title = {Learning from {Few} {Examples}: {A} {Summary} of {Approaches} to {Few}-{Shot} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Learning from {Few} {Examples}},
	url = {https://arxiv.org/abs/2203.04291},
	doi = {10.48550/ARXIV.2203.04291},
	abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
	urldate = {2023-12-31},
	author = {Parnami, Archit and Lee, Minwoo},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{o_mahony_one-shot_2019,
	title = {One-{Shot} {Learning} for {Custom} {Identification} {Tasks}; {A} {Review}},
	volume = {38},
	issn = {23519789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2351978920300263},
	doi = {10.1016/j.promfg.2020.01.025},
	language = {en},
	urldate = {2023-12-31},
	journal = {Procedia Manufacturing},
	author = {O‚Äô Mahony, N. and Campbell, Sean and Carvalho, Anderson and Krpalkova, L. and Hernandez, Gustavo Velasco and Harapanahalli, Suman and Riordan, D. and Walsh, J.},
	year = {2019},
	pages = {186--193},
}

@article{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2201.11903},
	doi = {10.48550/ARXIV.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-12-31},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 6},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}
